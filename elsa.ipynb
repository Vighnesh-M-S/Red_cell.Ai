{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # TensorFlow library for deep learning\n",
    "# Importing various pre-trained models from Keras applications\n",
    "from tensorflow.keras.applications import (VGG16, Xception, InceptionV3, ResNet50,\n",
    "                                           EfficientNetB0, EfficientNetB1, EfficientNetB2,\n",
    "                                           EfficientNetB3, EfficientNetB4, EfficientNetB5,\n",
    "                                           EfficientNetB6, EfficientNetB7)\n",
    "# Importing necessary modules from TensorFlow and Keras\n",
    "from tensorflow.keras import layers, regularizers  # Layers and regularization techniques\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Training callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Image data preprocessing\n",
    "# Importing other essential libraries for data handling and evaluation\n",
    "import numpy as np  # Numerical operations library\n",
    "import os  # Operating system related functionality\n",
    "from glob import glob  # File searching and pattern matching\n",
    "import shutil  # High-level file operations\n",
    "import zipfile  # ZIP file operations\n",
    "import random  # Random number generation\n",
    "import pandas as pd  # Data manipulation library\n",
    "import matplotlib.pyplot as plt  # Visualization library\n",
    "from tensorflow.keras.callbacks import Callback  # Base class for Keras callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_paths = glob(\"/content/drive/MyDrive/Elsafty_RBCs_Cellular_Images_and_Masks/Segmented images/*.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save_model_at          = \"/content/drive/MyDrive\"  # Directory to save the trained model\n",
    "Model_name             = \"Classifier\"  # Name for the model\n",
    "Transfer_Larning_Model = EfficientNetB0  # Transfer learning model choice (EfficientNetB0, VGG16, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shuffle_Before_Split   = \"no\"  # Flag indicating whether to shuffle data before split (\"no\" or \"yes\")\n",
    "\n",
    "selected_split_part    = 1  # Selected part of the images to be used for testing subset\n",
    "num_split_parts        = 6  # Total number of parts of the images for dataset splitting\n",
    "Give_data_balance      = \"no\"  # Flag indicating whether to balance data (\"no\" or \"yes\")\n",
    "batch_size             = 32  # Batch size for training\n",
    "Epochs_number          = 20  # Number of epochs for training\n",
    "initial_LearningRate   = 0.000004  # Initial learning rate for the optimizer\n",
    "Patience               = 3  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "min_delta              = 0.01  # Minimum change in monitored quantity for reducing the learning rate\n",
    "image_width            = 80  # Width of images\n",
    "image_height           = 80  # Height of images\n",
    "\n",
    "# Slides to be excluded in this experiment (used for Leave-One-Out experiments)\n",
    "Slides_to_be_excluded = [\"nothing to be excluded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "!pip install xlsxwriter\n",
    "\n",
    "import tensorflow as tf  # TensorFlow library for deep learning\n",
    "# Importing various pre-trained models from Keras applications\n",
    "from tensorflow.keras.applications import (VGG16, Xception, InceptionV3, ResNet50,\n",
    "                                           EfficientNetB0, EfficientNetB1, EfficientNetB2,\n",
    "                                           EfficientNetB3, EfficientNetB4, EfficientNetB5,\n",
    "                                           EfficientNetB6, EfficientNetB7)\n",
    "# Importing necessary modules from TensorFlow and Keras\n",
    "from tensorflow.keras import layers, regularizers  # Layers and regularization techniques\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Training callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Image data preprocessing\n",
    "# Importing other essential libraries for data handling and evaluation\n",
    "import numpy as np  # Numerical operations library\n",
    "import os  # Operating system related functionality\n",
    "from glob import glob  # File searching and pattern matching\n",
    "import shutil  # High-level file operations\n",
    "import zipfile  # ZIP file operations\n",
    "import random  # Random number generation\n",
    "import pandas as pd  # Data manipulation library\n",
    "import matplotlib.pyplot as plt  # Visualization library\n",
    "from google.colab import files  # Files module from Google Colab\n",
    "from tensorflow.keras.callbacks import Callback  # Base class for Keras callbacks\n",
    "\n",
    "########################################################################################################\n",
    "##########\n",
    "# Set initial configuration values\n",
    "\n",
    "# Get paths to ZIP files containing images\n",
    "compressed_paths = glob(\"/content/drive/MyDrive/Elsafty_RBCs_Cellular_Images_and_Masks/Segmented images/*.zip\")\n",
    "\n",
    "# Configuration parameters for model training and evaluation\n",
    "Save_model_at          = \"/content/drive/MyDrive\"  # Directory to save the trained model\n",
    "Model_name             = \"Classifier\"  # Name for the model\n",
    "Transfer_Larning_Model = EfficientNetB0  # Transfer learning model choice (EfficientNetB0, VGG16, etc.)\n",
    "\n",
    "### Please note that on selecting the option \"Shuffle_Before_Split\", the total images of each class in its folder will be randomly shuffled.\n",
    "### Subsequently, these shuffled images will be divided into a number of fully separated parts.\n",
    "### One-part will be allocated to testing, the second to validation, and the remaining part(s) to training.\n",
    "### Therefore, no cell will be present in more than one-part, and each part will contain images from each slide/patient.\n",
    "### Moreover, the shuffling will be performed with a fixed seed \"order\" to ensure consistency when choosing different parts for testing and validation.\n",
    "### Thus, the code will split the dataset with no data mix-up or allocating images from certain slides/patients to be specific for certain subset.\n",
    "### This method is useful for exploring data consistency but is not ideal for generalizing performance.\n",
    "### Conversely, without shuffling, the splitting will result in better performance generalization because validation and testing will be conducted on different cases.\n",
    "Shuffle_Before_Split   = \"no\"  # Flag indicating whether to shuffle data before split (\"no\" or \"yes\")\n",
    "\n",
    "selected_split_part    = 1  # Selected part of the images to be used for testing subset\n",
    "num_split_parts        = 6  # Total number of parts of the images for dataset splitting\n",
    "Give_data_balance      = \"no\"  # Flag indicating whether to balance data (\"no\" or \"yes\")\n",
    "batch_size             = 32  # Batch size for training\n",
    "Epochs_number          = 20  # Number of epochs for training\n",
    "initial_LearningRate   = 0.000004  # Initial learning rate for the optimizer\n",
    "Patience               = 3  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "min_delta              = 0.01  # Minimum change in monitored quantity for reducing the learning rate\n",
    "image_width            = 80  # Width of images\n",
    "image_height           = 80  # Height of images\n",
    "\n",
    "# Slides to be excluded in this experiment (used for Leave-One-Out experiments)\n",
    "Slides_to_be_excluded = [\"nothing to be excluded\"]\n",
    "# [\"nothing to be excluded\"]                                                            # Exclude nothing\n",
    "# [\"Slide 1 \", \"Slide 5 \", \"Slide 6 \", \"Slide 8 \", \"Slide 25\"]                          #  Source 1 slides\n",
    "# [\"Slide 2 \", \"Slide 3 \", \"Slide 4 \", \"Slide 7 \", \"Slide 9 \", \"Slide 11\"]              #  Source 2 slides\n",
    "# [\"Slide 14\", \"Slide 15\", \"Slide 19\", \"Slide 20\", \"Slide 22\", \"Slide 23\", \"Slide 24\"]  #  Source 3 slides\n",
    "# [\"Slide 10\", \"Slide 12\", \"Slide 13\", \"Slide 16\", \"Slide 17\", \"Slide 18\", \"Slide 21\"]  #  Source 4 slides\n",
    "\n",
    "##########\n",
    "########################################################################################################\n",
    "\n",
    "# Calculate the number of classes and extract class names from ZIP file paths\n",
    "num_classes = len(compressed_paths)\n",
    "classes = [path.split('/')[-1].split('.zip')[0] for path in compressed_paths]\n",
    "classes = sorted(classes)\n",
    "\n",
    "# Define directories for training, validation, test, and dataset root\n",
    "training_directory = \"/content/Training\"\n",
    "val_directory = \"/content/Validation\"\n",
    "test_directory = \"/content/Test\"\n",
    "root = \"/content/Decompressed_dataset/\"\n",
    "\n",
    "# Display separator for clarity\n",
    "print(\"#######################################################################\")\n",
    "\n",
    "# Display configuration information\n",
    "print(\"Model_name = \", Model_name + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(num_split_parts))\n",
    "print(\"Test_split_part = \", selected_split_part, \" Total_split_parts = \", num_split_parts)\n",
    "print(\"Shuffle_Before_Split = \", Shuffle_Before_Split)\n",
    "if str(Give_data_balance).lower() != \"no\":\n",
    "    print(\"Give_data_balance = \", Give_data_balance)\n",
    "print(\"Batch Size = \", batch_size)\n",
    "print(\"Epochs_number = \", Epochs_number)\n",
    "print(\"initial_LearningRate = \", initial_LearningRate)\n",
    "print(\"Patience = \", Patience, \" min Delta = \", min_delta)\n",
    "print(\"Images width = \", image_width, \" Images height = \", image_height)\n",
    "print(\"num_classes = \", num_classes)\n",
    "\n",
    "# Attempt to remove the 'root' directory if it exists\n",
    "# This step removes previously extracted data\n",
    "try:\n",
    "    shutil.rmtree(root[:-1])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Function to check if filename contains any of the specified slides\n",
    "def contains_string(filename, strings):\n",
    "    for s in strings:\n",
    "        if s in filename:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Extract files from each ZIP file\n",
    "for compressed_file in compressed_paths:\n",
    "    with zipfile.ZipFile(compressed_file, \"r\") as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            if not contains_string(file_info.filename, Slides_to_be_excluded):\n",
    "                zip_ref.extract(file_info, path=root)\n",
    "\n",
    "# Remove 'training_directory', 'val_directory', and 'test_directory' if they exist\n",
    "# This step clears existing directories for fresh data organization\n",
    "try:\n",
    "    shutil.rmtree(training_directory)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(val_directory)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(test_directory)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Organize images into training, validation, and test directories.\n",
    "# Iterate through each class and distribute images based on the specified number of split parts.\n",
    "### Please note that on selecting the option \"Shuffle_Before_Split\", the total images of each class in its folder will be randomly shuffled.\n",
    "### Subsequently, these shuffled images will be divided into a number of fully separated parts.\n",
    "### One-part will be allocated to testing, the second to validation, and the remaining part(s) to training.\n",
    "### Therefore, no cell will be present in more than one-part, and each part will contain images from each slide/patient.\n",
    "### Moreover, the shuffling will be performed with a fixed seed \"order\" to ensure consistency when choosing different parts for testing and validation.\n",
    "### Thus, the code will split the dataset with no data mix-up or allocating images from certain slides/patients to be specific for certain subset.\n",
    "### This method is useful for exploring data consistency but is not ideal for generalizing performance.\n",
    "### Conversely, without shuffling, the splitting will result in better performance generalization because validation and testing will be conducted on different cases.\n",
    "for cls in classes:\n",
    "    # Create directories for each class within 'training_directory', 'val_directory', and 'test_directory'\n",
    "    os.makedirs(os.path.join(training_directory + \"/\", cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_directory + \"/\", cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_directory + \"/\", cls), exist_ok=True)\n",
    "\n",
    "    # Define the folder containing images for the current class\n",
    "    class_folder = os.path.join(root, cls)\n",
    "\n",
    "    # List all files in the class folder and shuffle their order\n",
    "    allfiles = os.listdir(class_folder)\n",
    "\n",
    "    # Shuffle images before splitting\n",
    "    if str(Shuffle_Before_Split).lower() != \"no\":\n",
    "        random.seed(1234)\n",
    "        random.shuffle(allfiles)\n",
    "\n",
    "    # Divide images into parts\n",
    "    part_length = len(allfiles) // num_split_parts\n",
    "    remainder = len(allfiles) % num_split_parts\n",
    "    start_index = 0\n",
    "    result = []\n",
    "\n",
    "    # Create split parts with approximately equal sizes\n",
    "    for _ in range(num_split_parts):\n",
    "        end_index = start_index + part_length\n",
    "        if remainder > 0:\n",
    "            end_index += 1\n",
    "            remainder -= 1\n",
    "        result.append(allfiles[start_index:end_index])\n",
    "        start_index = end_index\n",
    "\n",
    "    # Determine test and validation indices based on selected part\n",
    "    test_inx = selected_split_part - 1\n",
    "    val_inx = selected_split_part\n",
    "    if val_inx >= num_split_parts:\n",
    "        val_inx = 0\n",
    "    test_files = result[test_inx]\n",
    "    val_files = result[val_inx]\n",
    "\n",
    "    # Separate files for training\n",
    "    new_lst = [x for x in allfiles if x not in test_files]\n",
    "    train_files = [x for x in new_lst if x not in val_files]\n",
    "\n",
    "    # Copy images to respective directories for training, validation, and test\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(test_directory + \"/\", cls, file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(val_directory + \"/\", cls, file))\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(training_directory + \"/\", cls, file))\n",
    "    print(\"bring class\", cls)  # Print the class name to indicate progress\n",
    "\n",
    "# Function to count the number of files in a directory\n",
    "def count_files(folder_path):\n",
    "    count = 0\n",
    "    for root, dirs, allfiles in os.walk(folder_path):\n",
    "        count += len(allfiles)\n",
    "    return count\n",
    "\n",
    "# Calculate and print the number of files in the training directory\n",
    "folder_path = training_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Training allfiles =\", file_count)\n",
    "\n",
    "# Calculate and print the number of files in the validation directory\n",
    "folder_path = val_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Validation allfiles =\", file_count)\n",
    "\n",
    "# Calculate and print the number of files in the test directory\n",
    "folder_path = test_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Test allfiles =\", file_count)\n",
    "\n",
    "# Display a separator for clarity\n",
    "print(\"#######################################################################\")\n",
    "\n",
    "# Define the size of the images\n",
    "image_size = (image_height, image_width)\n",
    "# Seed for data reproducibility\n",
    "seed = 123\n",
    "\n",
    "# Define data augmentation for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=360,  # Rotate images up to 360 degrees\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    vertical_flip=True  # Flip images vertically\n",
    ")\n",
    "\n",
    "# Data generators for the validation, test, and full data sets are initialized without augmentation\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "fullData_datagen = ImageDataGenerator()\n",
    "\n",
    "# Generating data batches from the directory for training, validation, and test sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_directory,  # Directory containing training images\n",
    "    target_size=image_size,  # Resize images to match 'image_size'\n",
    "    batch_size=batch_size,  # Number of samples per gradient update\n",
    "    class_mode='sparse',  # Mode for class labels (integer)\n",
    "    seed=seed  # Seed for shuffling\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_directory,  # Directory containing validation images\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_directory,  # Directory containing test images\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False  # Do not shuffle the test data during inference\n",
    ")\n",
    "\n",
    "# Use EfficientNetB0 as the base model for transfer learning\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    weights='imagenet',  # Initialize with pre-trained ImageNet weights\n",
    "    include_top=False,  # Exclude the fully connected layers from the top\n",
    "    input_shape=(image_height, image_width, 3)  # Define input image dimensions and channels\n",
    ")\n",
    "\n",
    "# Freeze the pre-trained layers to prevent updating during training\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True  # Setting the layers to trainable\n",
    "\n",
    "# Add a custom classification head on top of the pre-trained model\n",
    "preprocess_input = tf.keras.applications.efficientnet.preprocess_input  # Preprocessing input function for EfficientNet\n",
    "inputs = tf.keras.Input(shape=(image_height, image_width, 3))  # Define input shape\n",
    "x = preprocess_input(inputs)  # Preprocess the input\n",
    "x = base_model(x, training=False)  # Utilize the base model without training it\n",
    "x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  # Dropout layer with dropout rate of 0.2\n",
    "outputs = tf.keras.layers.Dense(num_classes)(x)  # Dense layer for classification\n",
    "model = tf.keras.Model(inputs, outputs)  # Create the final model\n",
    "\n",
    "# Display the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model with specified optimizer, loss, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_LearningRate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Sparse categorical cross-entropy loss\n",
    "    ],\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')  # Sparse categorical accuracy metric\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save the model after each epoch if it performs better\n",
    "save_model_after_each_epoch = tf.keras.callbacks.ModelCheckpoint(\n",
    "    (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(num_split_parts) + '.h5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True  # Save only the best model\n",
    ")\n",
    "\n",
    "# Define the learning rate reduction callback based on validation loss\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=Patience,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1,\n",
    "    min_delta=min_delta  # Minimum change in the monitored quantity to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Compute class weights based on the training dataset distribution\n",
    "class_counts = np.bincount(train_generator.classes)  # Count occurrences of each class in the training set\n",
    "total_samples = sum(class_counts)  # Calculate the total number of samples\n",
    "class_weights = [total_samples / (len(classes) * count) for count in class_counts]  # Compute class weights\n",
    "class_weights_dict = {class_label: weight for class_label, weight in enumerate(class_weights)}  # Create a dictionary of class weights\n",
    "\n",
    "# Check if data balancing is required and print class counts and weights if 'yes'\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    print(class_counts)\n",
    "    print(class_weights)\n",
    "\n",
    "# Train the model using the configured generators and settings\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size  # Calculate steps per epoch\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size  # Calculate validation steps\n",
    "\n",
    "# Define TensorBoard\n",
    "class CustomTensorBoardCallback_Accuracy(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        super(CustomTensorBoardCallback_Accuracy, self).__init__()\n",
    "        # Initializing the callback with a list of metrics to monitor\n",
    "        self.metrics = metrics\n",
    "        # Creating empty lists to store metric values, epochs, and learning rates\n",
    "        self.metric_values = {metric: [] for metric in metrics}\n",
    "        self.epochs = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        pass  # No action needed when the training starts\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Adding learning rates at the beginning of each epoch\n",
    "        self.learning_rates.append(self.model.optimizer.lr.numpy())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.epochs.append(epoch + 1)  # Incrementing the epoch number by 1\n",
    "        for metric in self.metrics:\n",
    "            # Collecting metric values at the end of each epoch\n",
    "            self.metric_values[metric].append(logs.get(metric))\n",
    "\n",
    "        # Plotting metrics and learning rates after each epoch\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))  # Creating a plot figure\n",
    "        lines = []\n",
    "        colors = ['blue', 'orange', 'green', 'red']  # Colors for different metrics\n",
    "\n",
    "        # Plotting each metric curve\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            line, = ax1.plot(self.epochs, self.metric_values[metric], '-', color=colors[i], label=metric)\n",
    "            lines.append(line)\n",
    "\n",
    "        # Adding a line for the learning rate\n",
    "        lr_line, = ax1.plot(self.epochs, self.learning_rates, '--', label='Learning Rate')\n",
    "\n",
    "        # Setting up plot labels, title, ticks, and legend\n",
    "        ax1.set_ylabel('Value')\n",
    "        ax1.set_title('Epoch')\n",
    "        ax1.set_xticks(np.arange(min(self.epochs), max(self.epochs) + 1, 1))\n",
    "        ax1.set_ylim(0, 1.0)\n",
    "        ax1.set_yticks(np.arange(0, 1.1, 0.05))\n",
    "        ax1.legend(lines + [lr_line], self.metrics + ['Learning Rate'])\n",
    "        ax1.grid(True)\n",
    "        ax1.tick_params(axis='x', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "        # Adding a secondary y-axis for better readability\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylim(0, 1.0)\n",
    "        ax2.set_yticks(np.arange(0, 1.1, 0.05))\n",
    "        ax2.yaxis.tick_right()\n",
    "        ax2.yaxis.set_label_position(\"right\")\n",
    "        ax2.set_ylabel('Value')\n",
    "\n",
    "        # Adding text labels for curves with an offset\n",
    "        offset = 0.02\n",
    "        for i in range(len(self.epochs)):\n",
    "            x = self.epochs[i]\n",
    "            y_lr = self.learning_rates[i]\n",
    "            y_val_loss = self.metric_values['val_loss'][i]\n",
    "            y_val_acc = self.metric_values['val_accuracy'][i]\n",
    "            lr_label = f'{y_lr:.1e}'\n",
    "            lr_label = (lr_label.replace(\"e-0\", \"e-\").replace(\"0e\", \"e\"))[2:]\n",
    "            val_loss_label = str(int(y_val_loss * 100))\n",
    "            val_acc_label = str(int(y_val_acc * 100))\n",
    "            ax1.text(x, y_lr - (offset * 4), lr_label, ha='center', va='bottom', color=lr_line.get_color())\n",
    "            ax1.text(x, y_val_loss - (offset * 3), val_loss_label, ha='center', va='bottom', color=colors[3])\n",
    "            ax1.text(x, y_val_acc + offset, val_acc_label, ha='center', va='bottom', color=colors[0])\n",
    "        plt.show()\n",
    "\n",
    "# List of metrics to monitor during training\n",
    "metrics = ['val_accuracy', 'accuracy', 'loss', 'val_loss']\n",
    "\n",
    "# Creating an instance of the CustomTensorBoardCallback_Accuracy class\n",
    "custom_tensorboard_callback_accuracy = CustomTensorBoardCallback_Accuracy(metrics)\n",
    "\n",
    "# Train the model with or without data balancing\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    # Train the model with data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights_dict,  # Apply class weights during training\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "else:\n",
    "    # Train the model without data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "\n",
    "# Load the test dataset from the directory using the specified configurations\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    str(test_directory) + \"/\",\n",
    "    labels=None,\n",
    "    shuffle=False,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset using the trained model\n",
    "rounded_predictions2 = []\n",
    "for image in test_dataset:\n",
    "    predictions = model.predict(image)  # Get predictions for each image in the test dataset\n",
    "    rounded_predictions = np.argmax(predictions, axis=1)  # Get the class index with the highest probability\n",
    "    rounded_predictions2.append(rounded_predictions)  # Append the predictions to a list\n",
    "rounded_predictions2 = np.concatenate(rounded_predictions2)  # Concatenate all predictions into a single array\n",
    "\n",
    "# Map predicted class indices to their corresponding class names\n",
    "class_nameslist = []\n",
    "for i in rounded_predictions2:\n",
    "    class_nameslist.append(classes[i])  # Map the predicted indices to their respective class names\n",
    "\n",
    "# Get file paths and true class labels for the test dataset\n",
    "XX = test_dataset.file_paths\n",
    "file_pathss = pd.DataFrame(XX, columns=['Path'])  # Create a DataFrame for file paths\n",
    "unique_word_list = []\n",
    "for path in XX:\n",
    "    path = path.replace(\"/\", \"\\\\\")\n",
    "    unique_word = path.split(\"\\\\\")[-2]  # Extract the true class label from the file path\n",
    "    unique_word_list.append(unique_word)  # Append true class labels to a list\n",
    "True_class_lsit = pd.DataFrame(unique_word_list, columns=['True Class'])  # Create a DataFrame for true class labels\n",
    "\n",
    "# Create DataFrames for predicted and true class labels along with file paths\n",
    "class_nameslist = pd.DataFrame(class_nameslist, columns=['Predicted Class'])\n",
    "Classification_result = pd.concat([file_pathss, class_nameslist, True_class_lsit], axis=1)\n",
    "\n",
    "# Identify misclassified samples by comparing predicted and true class labels\n",
    "misclassified_result = Classification_result[\n",
    "    Classification_result['Predicted Class'] != Classification_result['True Class']]\n",
    "\n",
    "# Save misclassified samples to an Excel file\n",
    "filename = (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(\n",
    "    num_split_parts) + \"_TestData.xlsx\")\n",
    "writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "misclassified_result.to_excel(writer, sheet_name='Misclassified_Test_Data', index=False)  # Write misclassified data to Excel\n",
    "writer.save()  # Save the Excel file\n",
    "\n",
    "files.download(filename)  # Download the Excel file\n",
    "print(\"************************ Test Dataset classification is completed and saved.\")  # Print completion message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
