{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting xlsxwriter\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xlsxwriter\n",
      "Successfully installed xlsxwriter-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf  # TensorFlow library for deep learning\n",
    "# Importing various pre-trained models from Keras applications\n",
    "from tensorflow.keras.applications import (VGG16, Xception, InceptionV3, ResNet50,\n",
    "                                           EfficientNetB0, EfficientNetB1, EfficientNetB2,\n",
    "                                           EfficientNetB3, EfficientNetB4, EfficientNetB5,\n",
    "                                           EfficientNetB6, EfficientNetB7)\n",
    "# Importing necessary modules from TensorFlow and Keras\n",
    "from tensorflow.keras import layers, regularizers  # Layers and regularization techniques\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Training callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Image data preprocessing\n",
    "# Importing other essential libraries for data handling and evaluation\n",
    "import numpy as np  # Numerical operations library\n",
    "import os  # Operating system related functionality\n",
    "from glob import glob  # File searching and pattern matching\n",
    "import shutil  # High-level file operations\n",
    "import zipfile  # ZIP file operations\n",
    "import random  # Random number generation\n",
    "import pandas as pd  # Data manipulation library\n",
    "import matplotlib.pyplot as plt  # Visualization library\n",
    "from tensorflow.keras.callbacks import Callback  # Base class for Keras callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_paths = glob(\"DataSets/Elsafty_RBCs_for_Classification.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Save_model_at          = \"Classification\"  # Directory to save the trained model\n",
    "Model_name             = \"Classifier\"  # Name for the model\n",
    "Transfer_Larning_Model = EfficientNetB0  # Transfer learning model choice (EfficientNetB0, VGG16, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shuffle_Before_Split   = \"no\"  # Flag indicating whether to shuffle data before split (\"no\" or \"yes\")\n",
    "\n",
    "selected_split_part    = 1  # Selected part of the images to be used for testing subset\n",
    "num_split_parts        = 6  # Total number of parts of the images for dataset splitting\n",
    "Give_data_balance      = \"no\"  # Flag indicating whether to balance data (\"no\" or \"yes\")\n",
    "batch_size             = 32  # Batch size for training\n",
    "Epochs_number          = 20  # Number of epochs for training\n",
    "initial_LearningRate   = 0.000004  # Initial learning rate for the optimizer\n",
    "Patience               = 3  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "min_delta              = 0.01  # Minimum change in monitored quantity for reducing the learning rate\n",
    "image_width            = 80  # Width of images\n",
    "image_height           = 80  # Height of images\n",
    "\n",
    "# Slides to be excluded in this experiment (used for Leave-One-Out experiments)\n",
    "Slides_to_be_excluded = [\"nothing to be excluded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of classes and extract class names from ZIP file paths\n",
    "num_classes = len(compressed_paths)\n",
    "classes = [path.split('/')[-1].split('.zip')[0] for path in compressed_paths]\n",
    "classes = sorted(classes)\n",
    "\n",
    "# Define directories for training, validation, test, and dataset root\n",
    "training_directory = \"Classification/Training\"\n",
    "val_directory = \"Classification/Validation\"\n",
    "test_directory = \"Classification/Test\"\n",
    "root = \"Classification/Decompressed_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_name =  Classifier_Split_Part_1_from_6\n",
      "Test_split_part =  1  Total_split_parts =  6\n",
      "Shuffle_Before_Split =  no\n",
      "Batch Size =  32\n",
      "Epochs_number =  20\n",
      "initial_LearningRate =  4e-06\n",
      "Patience =  3  min Delta =  0.01\n",
      "Images width =  80  Images height =  80\n",
      "num_classes =  1\n"
     ]
    }
   ],
   "source": [
    "# Display configuration information\n",
    "print(\"Model_name = \", Model_name + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(num_split_parts))\n",
    "print(\"Test_split_part = \", selected_split_part, \" Total_split_parts = \", num_split_parts)\n",
    "print(\"Shuffle_Before_Split = \", Shuffle_Before_Split)\n",
    "if str(Give_data_balance).lower() != \"no\":\n",
    "    print(\"Give_data_balance = \", Give_data_balance)\n",
    "print(\"Batch Size = \", batch_size)\n",
    "print(\"Epochs_number = \", Epochs_number)\n",
    "print(\"initial_LearningRate = \", initial_LearningRate)\n",
    "print(\"Patience = \", Patience, \" min Delta = \", min_delta)\n",
    "print(\"Images width = \", image_width, \" Images height = \", image_height)\n",
    "print(\"num_classes = \", num_classes)\n",
    "\n",
    "# Attempt to remove the 'root' directory if it exists\n",
    "# This step removes previously extracted data\n",
    "try:\n",
    "    shutil.rmtree(root[:-1])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vighneshms/Downloads/Red_cell.Ai\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if filename contains any of the specified slides\n",
    "def contains_string(filename, strings):\n",
    "    for s in strings:\n",
    "        if s in filename:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Extract files from each ZIP file\n",
    "for compressed_file in compressed_paths:\n",
    "    with zipfile.ZipFile(compressed_file, \"r\") as zip_ref:\n",
    "        for file_info in zip_ref.infolist():\n",
    "            if not contains_string(file_info.filename, Slides_to_be_excluded):\n",
    "                zip_ref.extract(file_info, path=root)\n",
    "\n",
    "# Remove 'training_directory', 'val_directory', and 'test_directory' if they exist\n",
    "# This step clears existing directories for fresh data organization\n",
    "try:\n",
    "    shutil.rmtree(training_directory)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(val_directory)\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    shutil.rmtree(test_directory)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Elsafty_RBCs_for_Classification' processing complete.\n"
     ]
    }
   ],
   "source": [
    "classes = [cls for cls in os.listdir(root) if os.path.isdir(os.path.join(root, cls))]\n",
    "\n",
    "for cls in classes:\n",
    "    # Define the folder containing images for the current class\n",
    "    class_folder = os.path.join(root, cls)\n",
    "\n",
    "    # List all files in the class folder and exclude directories\n",
    "    allfiles = [f for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n",
    "    if not allfiles:\n",
    "        print(f\"No files found in {class_folder}\")\n",
    "        continue\n",
    "\n",
    "    # Shuffle images before splitting\n",
    "    if str(Shuffle_Before_Split).lower() != \"no\":\n",
    "        random.seed(1234)\n",
    "        random.shuffle(allfiles)\n",
    "\n",
    "    # Split files into training, validation, and test parts\n",
    "    part_length = len(allfiles) // num_split_parts\n",
    "    remainder = len(allfiles) % num_split_parts\n",
    "    start_index = 0\n",
    "    result = []\n",
    "\n",
    "    for _ in range(num_split_parts):\n",
    "        end_index = start_index + part_length\n",
    "        if remainder > 0:\n",
    "            end_index += 1\n",
    "            remainder -= 1\n",
    "        result.append(allfiles[start_index:end_index])\n",
    "        start_index = end_index\n",
    "\n",
    "    # Test, validation, and training files\n",
    "    test_inx = selected_split_part - 1\n",
    "    val_inx = (selected_split_part % num_split_parts)\n",
    "    test_files = result[test_inx]\n",
    "    val_files = result[val_inx]\n",
    "    train_files = [x for x in allfiles if x not in test_files + val_files]\n",
    "\n",
    "    # Create directories for training, validation, and test\n",
    "    for directory in [training_directory, val_directory, test_directory]:\n",
    "        os.makedirs(os.path.join(directory, cls), exist_ok=True)\n",
    "\n",
    "    # Copy files\n",
    "    for file_group, dest_dir in [(test_files, test_directory), (val_files, val_directory), (train_files, training_directory)]:\n",
    "        for file in file_group:\n",
    "            src = os.path.join(class_folder, file)\n",
    "            dest = os.path.join(dest_dir, cls, file)\n",
    "            try:\n",
    "                shutil.copy(src, dest)\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying {file} for class {cls}: {e}\")\n",
    "\n",
    "    print(f\"Class '{cls}' processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'SEGMENTED - Class 7 - Teardrops' processing complete. Training: 10864, Validation: 2717, Test: 2717\n",
      "Class 'SEGMENTED - Class 6 - Burr Cells' processing complete. Training: 5964, Validation: 1492, Test: 1492\n",
      "Class 'SEGMENTED - Class 5 - Three Overlapping RBCs' processing complete. Training: 10384, Validation: 2596, Test: 2597\n",
      "Class 'SEGMENTED - Class 2 - Ovalocytes' processing complete. Training: 36715, Validation: 9179, Test: 9179\n",
      "Class 'SEGMENTED - Class 8 - Angled Cells' processing complete. Training: 16124, Validation: 4031, Test: 4032\n",
      "Class 'SEGMENTED - Class 1 - Rounded RBCs' processing complete. Training: 30892, Validation: 7723, Test: 7723\n",
      "Class 'SEGMENTED - Class 3 - Fragmented RBCs' processing complete. Training: 4790, Validation: 1198, Test: 1198\n",
      "Class 'SEGMENTED - Class 9 - Borderline Ovalocytes' processing complete. Training: 23692, Validation: 5924, Test: 5924\n",
      "Class 'SEGMENTED - Class 4 - Two Overlapping RBCs' processing complete. Training: 20906, Validation: 5227, Test: 5227\n",
      "Files organized in Organized_Dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "source_root = \"Classification/Decompressed_dataset/Elsafty_RBCs_for_Classification\"  # Source folder containing class folders\n",
    "destination_root = \"Organized_Dataset\"  # New root folder for organized dataset\n",
    "\n",
    "# Subdirectories for organized dataset\n",
    "training_directory = os.path.join(destination_root, \"Training\")\n",
    "validation_directory = os.path.join(destination_root, \"Validation\")\n",
    "test_directory = os.path.join(destination_root, \"Test\")\n",
    "\n",
    "# Create destination directories\n",
    "for directory in [training_directory, validation_directory, test_directory]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "num_split_parts = 6  # Number of parts to split the dataset into\n",
    "selected_split_part = 1  # Part to use as the test set\n",
    "shuffle_before_split = True  # Whether to shuffle files before splitting\n",
    "\n",
    "# Get class folders\n",
    "classes = [cls for cls in os.listdir(source_root) if os.path.isdir(os.path.join(source_root, cls))]\n",
    "\n",
    "for cls in classes:\n",
    "    # Define paths for the current class\n",
    "    class_folder = os.path.join(source_root, cls)\n",
    "\n",
    "    # Get all files in the class folder\n",
    "    all_files = [f for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n",
    "    if not all_files:\n",
    "        print(f\"No files found in {class_folder}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Shuffle files if needed\n",
    "    if shuffle_before_split:\n",
    "        import random\n",
    "        random.seed(1234)\n",
    "        random.shuffle(all_files)\n",
    "\n",
    "    # Split files into parts\n",
    "    part_length = len(all_files) // num_split_parts\n",
    "    remainder = len(all_files) % num_split_parts\n",
    "    parts = []\n",
    "    start_index = 0\n",
    "\n",
    "    for _ in range(num_split_parts):\n",
    "        end_index = start_index + part_length + (1 if remainder > 0 else 0)\n",
    "        remainder -= 1 if remainder > 0 else 0\n",
    "        parts.append(all_files[start_index:end_index])\n",
    "        start_index = end_index\n",
    "\n",
    "    # Determine test, validation, and training files\n",
    "    test_files = parts[selected_split_part - 1]\n",
    "    validation_files = parts[selected_split_part % num_split_parts]\n",
    "    training_files = [f for f in all_files if f not in test_files + validation_files]\n",
    "\n",
    "    # Copy files to respective directories\n",
    "    for file_group, target_directory in [\n",
    "        (test_files, test_directory),\n",
    "        (validation_files, validation_directory),\n",
    "        (training_files, training_directory),\n",
    "    ]:\n",
    "        class_target_dir = os.path.join(target_directory, cls)\n",
    "        os.makedirs(class_target_dir, exist_ok=True)\n",
    "        for file in file_group:\n",
    "            src = os.path.join(class_folder, file)\n",
    "            dst = os.path.join(class_target_dir, file)\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "    print(f\"Class '{cls}' processing complete. Training: {len(training_files)}, Validation: {len(validation_files)}, Test: {len(test_files)}\")\n",
    "\n",
    "print(f\"Files organized in {destination_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'Classification/Decompressed_dataset/Elsafty_RBCs_for_Classification/SEGMENTED - Class 7 - Teardrops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Copy images to respective directories for training, validation, and test\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m test_files:\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_directory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m val_files:\n\u001b[1;32m     49\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_folder, file), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(val_directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m, file))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/shutil.py:418\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    417\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 418\u001b[0m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    262\u001b[0m     os\u001b[38;5;241m.\u001b[39msymlink(os\u001b[38;5;241m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fsrc, \u001b[38;5;28mopen\u001b[39m(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fdst:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# macOS\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'Classification/Decompressed_dataset/Elsafty_RBCs_for_Classification/SEGMENTED - Class 7 - Teardrops'"
     ]
    }
   ],
   "source": [
    "for cls in classes:\n",
    "    # Create directories for each class within 'training_directory', 'val_directory', and 'test_directory'\n",
    "    os.makedirs(os.path.join(training_directory + \"/\", cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_directory + \"/\", cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_directory + \"/\", cls), exist_ok=True)\n",
    "\n",
    "    # Define the folder containing images for the current class\n",
    "    class_folder = os.path.join(root, cls)\n",
    "\n",
    "    # List all files in the class folder and shuffle their order\n",
    "    allfiles = os.listdir(class_folder)\n",
    "\n",
    "    # Shuffle images before splitting\n",
    "    if str(Shuffle_Before_Split).lower() != \"no\":\n",
    "        random.seed(1234)\n",
    "        random.shuffle(allfiles)\n",
    "\n",
    "    # Divide images into parts\n",
    "    part_length = len(allfiles) // num_split_parts\n",
    "    remainder = len(allfiles) % num_split_parts\n",
    "    start_index = 0\n",
    "    result = []\n",
    "\n",
    "    # Create split parts with approximately equal sizes\n",
    "    for _ in range(num_split_parts):\n",
    "        end_index = start_index + part_length\n",
    "        if remainder > 0:\n",
    "            end_index += 1\n",
    "            remainder -= 1\n",
    "        result.append(allfiles[start_index:end_index])\n",
    "        start_index = end_index\n",
    "\n",
    "    # Determine test and validation indices based on selected part\n",
    "    test_inx = selected_split_part - 1\n",
    "    val_inx = selected_split_part\n",
    "    if val_inx >= num_split_parts:\n",
    "        val_inx = 0\n",
    "    test_files = result[test_inx]\n",
    "    val_files = result[val_inx]\n",
    "\n",
    "    # Separate files for training\n",
    "    new_lst = [x for x in allfiles if x not in test_files]\n",
    "    train_files = [x for x in new_lst if x not in val_files]\n",
    "\n",
    "    # Copy images to respective directories for training, validation, and test\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(test_directory + \"/\", cls, file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(val_directory + \"/\", cls, file))\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(training_directory + \"/\", cls, file))\n",
    "    print(\"bring class\", cls)  # Print the class name to indicate progress\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'Elsafty_RBCs_for_Classification' processing complete.\n"
     ]
    }
   ],
   "source": [
    "for cls in classes:\n",
    "    # Define the folder containing images for the current class\n",
    "    class_folder = os.path.join(root, cls)\n",
    "    if not os.path.exists(class_folder):\n",
    "        print(f\"Directory not found: {class_folder}\")\n",
    "        continue\n",
    "\n",
    "    # List all files in the class folder and exclude directories\n",
    "    allfiles = [f for f in os.listdir(class_folder) if os.path.isfile(os.path.join(class_folder, f))]\n",
    "    if not allfiles:\n",
    "        print(f\"No files found in {class_folder}\")\n",
    "        continue\n",
    "\n",
    "    # Shuffle images before splitting\n",
    "    if str(Shuffle_Before_Split).lower() != \"no\":\n",
    "        random.seed(1234)\n",
    "        random.shuffle(allfiles)\n",
    "\n",
    "    # Split files into training, validation, and test parts\n",
    "    part_length = len(allfiles) // num_split_parts\n",
    "    remainder = len(allfiles) % num_split_parts\n",
    "    start_index = 0\n",
    "    result = []\n",
    "\n",
    "    for _ in range(num_split_parts):\n",
    "        end_index = start_index + part_length\n",
    "        if remainder > 0:\n",
    "            end_index += 1\n",
    "            remainder -= 1\n",
    "        result.append(allfiles[start_index:end_index])\n",
    "        start_index = end_index\n",
    "\n",
    "    # Test, validation, and training files\n",
    "    test_inx = selected_split_part - 1\n",
    "    val_inx = selected_split_part % num_split_parts\n",
    "    test_files = result[test_inx]\n",
    "    val_files = result[val_inx]\n",
    "    train_files = [x for x in allfiles if x not in test_files + val_files]\n",
    "\n",
    "    # Copy images to respective directories\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(test_directory, cls, file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(val_directory, cls, file))\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(training_directory, cls, file))\n",
    "\n",
    "    print(f\"Class '{cls}' processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory path: Organized_Dataset/Training\n",
      "Validation directory path: Organized_Dataset/Validation\n",
      "Test directory path: Organized_Dataset/Test\n"
     ]
    }
   ],
   "source": [
    "print(\"Training directory path:\", training_directory)\n",
    "print(\"Validation directory path:\", validation_directory)\n",
    "print(\"Test directory path:\", test_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training directory path: Classification/Training\n",
      "Validation directory path: Classification/Validation\n",
      "Test directory path: Classification/Test\n",
      "Inspecting Classification/Training: 0 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 7 - Teardrops: 10864 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 6 - Burr Cells: 5964 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 5 - Three Overlapping RBCs: 10384 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 2 - Ovalocytes: 36715 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 8 - Angled Cells: 16124 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 1 - Rounded RBCs: 30892 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 3 - Fragmented RBCs: 4790 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 9 - Borderline Ovalocytes: 23692 files found\n",
      "Inspecting Classification/Training/SEGMENTED - Class 4 - Two Overlapping RBCs: 20906 files found\n",
      "Training allfiles = 160331\n",
      "Inspecting Classification/Validation: 0 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 7 - Teardrops: 2717 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 6 - Burr Cells: 1492 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 5 - Three Overlapping RBCs: 2596 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 2 - Ovalocytes: 9179 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 8 - Angled Cells: 4031 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 1 - Rounded RBCs: 7723 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 3 - Fragmented RBCs: 1198 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 9 - Borderline Ovalocytes: 5924 files found\n",
      "Inspecting Classification/Validation/SEGMENTED - Class 4 - Two Overlapping RBCs: 5227 files found\n",
      "Validation allfiles = 40087\n",
      "Inspecting Classification/Test: 0 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 7 - Teardrops: 2717 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 6 - Burr Cells: 1492 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 5 - Three Overlapping RBCs: 2597 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 2 - Ovalocytes: 9179 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 8 - Angled Cells: 4032 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 1 - Rounded RBCs: 7723 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 3 - Fragmented RBCs: 1198 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 9 - Borderline Ovalocytes: 5924 files found\n",
      "Inspecting Classification/Test/SEGMENTED - Class 4 - Two Overlapping RBCs: 5227 files found\n",
      "Test allfiles = 40089\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to count all files in a directory, including subdirectories\n",
    "def count_files(folder_path):\n",
    "    count = 0\n",
    "    for root, dirs, allfiles in os.walk(folder_path):\n",
    "        print(f\"Inspecting {root}: {len(allfiles)} files found\")  # Debugging line\n",
    "        count += len(allfiles)\n",
    "    return count\n",
    "\n",
    "# Updated directories\n",
    "training_directory = \"Classification/Training\"\n",
    "validation_directory = \"Classification/Validation\"\n",
    "test_directory = \"Classification/Test\"\n",
    "\n",
    "# Debugging: Print directory paths\n",
    "print(\"Training directory path:\", training_directory)\n",
    "print(\"Validation directory path:\", validation_directory)\n",
    "print(\"Test directory path:\", test_directory)\n",
    "\n",
    "# Calculate and print the number of files in the training directory\n",
    "training_file_count = count_files(training_directory)\n",
    "print(\"Training allfiles =\", training_file_count)\n",
    "\n",
    "# Calculate and print the number of files in the validation directory\n",
    "validation_file_count = count_files(validation_directory)\n",
    "print(\"Validation allfiles =\", validation_file_count)\n",
    "\n",
    "# Calculate and print the number of files in the test directory\n",
    "test_file_count = count_files(test_directory)\n",
    "print(\"Test allfiles =\", test_file_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training allfiles = 160331\n",
      "Validation allfiles = 40087\n",
      "Test allfiles = 40089\n"
     ]
    }
   ],
   "source": [
    "def count_files(folder_path):\n",
    "    count = 0\n",
    "    for root, dirs, allfiles in os.walk(folder_path):\n",
    "        count += len(allfiles)\n",
    "    return count\n",
    "\n",
    "# Calculate and print the number of files in the training directory\n",
    "folder_path = training_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Training allfiles =\", file_count)\n",
    "\n",
    "# Calculate and print the number of files in the validation directory\n",
    "folder_path = val_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Validation allfiles =\", file_count)\n",
    "\n",
    "# Calculate and print the number of files in the test directory\n",
    "folder_path = test_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Test allfiles =\", file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 160331 images belonging to 9 classes.\n",
      "Found 40087 images belonging to 9 classes.\n",
      "Found 40089 images belonging to 9 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,281</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m1,281\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,050,852</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,050,852\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,008,829</span> (15.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,008,829\u001b[0m (15.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,023</span> (164.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m42,023\u001b[0m (164.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the size of the images\n",
    "image_size = (image_height, image_width)\n",
    "# Seed for data reproducibility\n",
    "seed = 123\n",
    "\n",
    "# Define data augmentation for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=360,  # Rotate images up to 360 degrees\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    vertical_flip=True  # Flip images vertically\n",
    ")\n",
    "\n",
    "# Data generators for the validation, test, and full data sets are initialized without augmentation\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "fullData_datagen = ImageDataGenerator()\n",
    "\n",
    "# Generating data batches from the directory for training, validation, and test sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_directory,  # Directory containing training images\n",
    "    target_size=image_size,  # Resize images to match 'image_size'\n",
    "    batch_size=batch_size,  # Number of samples per gradient update\n",
    "    class_mode='sparse',  # Mode for class labels (integer)\n",
    "    seed=seed  # Seed for shuffling\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_directory,  # Directory containing validation images\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_directory,  # Directory containing test images\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False  # Do not shuffle the test data during inference\n",
    ")\n",
    "\n",
    "# Use EfficientNetB0 as the base model for transfer learning\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    weights='imagenet',  # Initialize with pre-trained ImageNet weights\n",
    "    include_top=False,  # Exclude the fully connected layers from the top\n",
    "    input_shape=(image_height, image_width, 3)  # Define input image dimensions and channels\n",
    ")\n",
    "\n",
    "# Freeze the pre-trained layers to prevent updating during training\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True  # Setting the layers to trainable\n",
    "\n",
    "# Add a custom classification head on top of the pre-trained model\n",
    "preprocess_input = tf.keras.applications.efficientnet.preprocess_input  # Preprocessing input function for EfficientNet\n",
    "inputs = tf.keras.Input(shape=(image_height, image_width, 3))  # Define input shape\n",
    "x = preprocess_input(inputs)  # Preprocess the input\n",
    "x = base_model(x, training=False)  # Utilize the base model without training it\n",
    "x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  # Dropout layer with dropout rate of 0.2\n",
    "outputs = tf.keras.layers.Dense(num_classes)(x)  # Dense layer for classification\n",
    "model = tf.keras.Model(inputs, outputs)  # Create the final model\n",
    "\n",
    "# Display the model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with specified optimizer, loss, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_LearningRate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Sparse categorical cross-entropy loss\n",
    "    ],\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')  # Sparse categorical accuracy metric\n",
    "    ]\n",
    ")\n",
    "\n",
    "save_model_after_each_epoch = tf.keras.callbacks.ModelCheckpoint(\n",
    "    (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(num_split_parts) + '.keras'),\n",
    "    verbose=1,\n",
    "    save_best_only=True  # Save only the best model\n",
    ")\n",
    "\n",
    "# Save the model after each epoch if it performs better\n",
    "# save_model_after_each_epoch = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(num_split_parts) + '.h5'),\n",
    "#     verbose=1,\n",
    "#     save_best_only=True  # Save only the best model\n",
    "# )\n",
    "\n",
    "# Define the learning rate reduction callback based on validation loss\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=Patience,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1,\n",
    "    min_delta=min_delta  # Minimum change in the monitored quantity to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Compute class weights based on the training dataset distribution\n",
    "class_counts = np.bincount(train_generator.classes)  # Count occurrences of each class in the training set\n",
    "total_samples = sum(class_counts)  # Calculate the total number of samples\n",
    "class_weights = [total_samples / (len(classes) * count) for count in class_counts]  # Compute class weights\n",
    "class_weights_dict = {class_label: weight for class_label, weight in enumerate(class_weights)}  # Create a dictionary of class weights\n",
    "\n",
    "# Check if data balancing is required and print class counts and weights if 'yes'\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    print(class_counts)\n",
    "    print(class_weights)\n",
    "\n",
    "# Train the model using the configured generators and settings\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size  # Calculate steps per epoch\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size  # Calculate validation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The filepath provided must end in `.keras` (Keras model format). Received: filepath=Classification/Classifier_Split_Part_1_from_6.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m      5\u001b[0m     loss\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Save the model after each epoch if it performs better\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m save_model_after_each_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelCheckpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSave_model_at\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_Split_Part_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mselected_split_part\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_from_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_split_parts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_best_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save only the best model\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Define the learning rate reduction callback based on validation loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m lr_reduction \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(\n\u001b[1;32m     22\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m     factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39mmin_delta  \u001b[38;5;66;03m# Minimum change in the monitored quantity to qualify as an improvement\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/callbacks/model_checkpoint.py:191\u001b[0m, in \u001b[0;36mModelCheckpoint.__init__\u001b[0;34m(self, filepath, monitor, verbose, save_best_only, save_weights_only, mode, save_freq, initial_value_threshold)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    192\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe filepath provided must end in `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Keras model format). Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: The filepath provided must end in `.keras` (Keras model format). Received: filepath=Classification/Classifier_Split_Part_1_from_6.h5"
     ]
    }
   ],
   "source": [
    "# Compile the model with specified optimizer, loss, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_LearningRate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Sparse categorical cross-entropy loss\n",
    "    ],\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')  # Sparse categorical accuracy metric\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save the model after each epoch if it performs better\n",
    "save_model_after_each_epoch = tf.keras.callbacks.ModelCheckpoint(\n",
    "    (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(num_split_parts) + '.h5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True  # Save only the best model\n",
    ")\n",
    "\n",
    "# Define the learning rate reduction callback based on validation loss\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=Patience,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1,\n",
    "    min_delta=min_delta  # Minimum change in the monitored quantity to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Compute class weights based on the training dataset distribution\n",
    "class_counts = np.bincount(train_generator.classes)  # Count occurrences of each class in the training set\n",
    "total_samples = sum(class_counts)  # Calculate the total number of samples\n",
    "class_weights = [total_samples / (len(classes) * count) for count in class_counts]  # Compute class weights\n",
    "class_weights_dict = {class_label: weight for class_label, weight in enumerate(class_weights)}  # Create a dictionary of class weights\n",
    "\n",
    "# Check if data balancing is required and print class counts and weights if 'yes'\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    print(class_counts)\n",
    "    print(class_weights)\n",
    "\n",
    "# Train the model using the configured generators and settings\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size  # Calculate steps per epoch\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size  # Calculate validation steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TensorBoard\n",
    "class CustomTensorBoardCallback_Accuracy(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        super(CustomTensorBoardCallback_Accuracy, self).__init__()\n",
    "        # Initializing the callback with a list of metrics to monitor\n",
    "        self.metrics = metrics\n",
    "        # Creating empty lists to store metric values, epochs, and learning rates\n",
    "        self.metric_values = {metric: [] for metric in metrics}\n",
    "        self.epochs = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        pass  # No action needed when the training starts\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Adding learning rates at the beginning of each epoch\n",
    "        self.learning_rates.append(self.model.optimizer.learning_rate.numpy())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.epochs.append(epoch + 1)  # Incrementing the epoch number by 1\n",
    "        for metric in self.metrics:\n",
    "            # Collecting metric values at the end of each epoch\n",
    "            self.metric_values[metric].append(logs.get(metric))\n",
    "\n",
    "        # Plotting metrics and learning rates after each epoch\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))  # Creating a plot figure\n",
    "        lines = []\n",
    "        colors = ['blue', 'orange', 'green', 'red']  # Colors for different metrics\n",
    "\n",
    "        # Plotting each metric curve\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            line, = ax1.plot(self.epochs, self.metric_values[metric], '-', color=colors[i], label=metric)\n",
    "            lines.append(line)\n",
    "\n",
    "        # Adding a line for the learning rate\n",
    "        lr_line, = ax1.plot(self.epochs, self.learning_rates, '--', label='Learning Rate')\n",
    "\n",
    "        # Setting up plot labels, title, ticks, and legend\n",
    "        ax1.set_ylabel('Value')\n",
    "        ax1.set_title('Epoch')\n",
    "        ax1.set_xticks(np.arange(min(self.epochs), max(self.epochs) + 1, 1))\n",
    "        ax1.set_ylim(0, 1.0)\n",
    "        ax1.set_yticks(np.arange(0, 1.1, 0.05))\n",
    "        ax1.legend(lines + [lr_line], self.metrics + ['Learning Rate'])\n",
    "        ax1.grid(True)\n",
    "        ax1.tick_params(axis='x', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "        # Adding a secondary y-axis for better readability\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylim(0, 1.0)\n",
    "        ax2.set_yticks(np.arange(0, 1.1, 0.05))\n",
    "        ax2.yaxis.tick_right()\n",
    "        ax2.yaxis.set_label_position(\"right\")\n",
    "        ax2.set_ylabel('Value')\n",
    "\n",
    "        # Adding text labels for curves with an offset\n",
    "        offset = 0.02\n",
    "        for i in range(len(self.epochs)):\n",
    "            x = self.epochs[i]\n",
    "            y_lr = self.learning_rates[i]\n",
    "            y_val_loss = self.metric_values['val_loss'][i]\n",
    "            y_val_acc = self.metric_values['val_accuracy'][i]\n",
    "            lr_label = f'{y_lr:.1e}'\n",
    "            lr_label = (lr_label.replace(\"e-0\", \"e-\").replace(\"0e\", \"e\"))[2:]\n",
    "            val_loss_label = str(int(y_val_loss * 100))\n",
    "            val_acc_label = str(int(y_val_acc * 100))\n",
    "            ax1.text(x, y_lr - (offset * 4), lr_label, ha='center', va='bottom', color=lr_line.get_color())\n",
    "            ax1.text(x, y_val_loss - (offset * 3), val_loss_label, ha='center', va='bottom', color=colors[3])\n",
    "            ax1.text(x, y_val_acc + offset, val_acc_label, ha='center', va='bottom', color=colors[0])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TensorBoard Callback\n",
    "class CustomTensorBoardCallback_Accuracy(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        super(CustomTensorBoardCallback_Accuracy, self).__init__()\n",
    "        # Initializing the callback with a list of metrics to monitor\n",
    "        self.metrics = metrics\n",
    "        # Creating empty lists to store metric values, epochs, and learning rates\n",
    "        self.metric_values = {metric: [] for metric in metrics}\n",
    "        self.epochs = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        pass  # No action needed when the training starts\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Correctly access learning rate in TensorFlow 2.11+\n",
    "        self.learning_rates.append(self.model.optimizer.learning_rate.numpy())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.epochs.append(epoch + 1)  # Increment the epoch number by 1\n",
    "        for metric in self.metrics:\n",
    "            # Collect metric values at the end of each epoch\n",
    "            self.metric_values[metric].append(logs.get(metric))\n",
    "\n",
    "        # Plotting metrics and learning rates after each epoch\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))  # Create a plot figure\n",
    "        lines = []\n",
    "        colors = ['blue', 'orange', 'green', 'red']  # Colors for different metrics\n",
    "\n",
    "        # Plot each metric curve\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            line, = ax1.plot(self.epochs, self.metric_values[metric], '-', color=colors[i], label=metric)\n",
    "            lines.append(line)\n",
    "\n",
    "        # Add a line for the learning rate\n",
    "        lr_line, = ax1.plot(self.epochs, self.learning_rates, '--', label='Learning Rate')\n",
    "\n",
    "        # Set up plot labels, title, ticks, and legend\n",
    "        ax1.set_ylabel('Value')\n",
    "        ax1.set_title('Training Progress per Epoch')\n",
    "        ax1.set_xticks(np.arange(min(self.epochs), max(self.epochs) + 1, 1))\n",
    "        ax1.legend(lines + [lr_line], self.metrics + ['Learning Rate'])\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Add a secondary y-axis for better readability\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Learning Rate')\n",
    "        ax2.tick_params(axis='y')\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 11:47:32.431789: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of 8 which is outside the valid range of [0, 1).  Label values: 6 0 6 1 1 8 5 4 6 1 1 0 5 4 0 8 1 1 1 0 1 7 3 7 1 8 1 1 0 0 6 1\n",
      "2024-11-08 11:47:32.431811: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Received a label value of 8 which is outside the valid range of [0, 1).  Label values: 6 0 6 1 1 8 5 4 6 1 1 0 5 4 0 8 1 1 1 0 1 7 3 7 1 8 1 1 0 0 6 1\n",
      "\t [[{{function_node __inference_one_step_on_data_58397}}{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/0d/6qdp6tjj2fj9mgn8w3n53sq40000gn/T/ipykernel_50689/1450207562.py\", line 21, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 359, in _compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 327, in compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 611, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 652, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/loss.py\", line 60, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 1870, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py\", line 1559, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py\", line 671, in sparse_categorical_crossentropy\n\nReceived a label value of 8 which is outside the valid range of [0, 1).  Label values: 6 0 6 1 1 8 5 4 6 1 1 0 5 4 0 8 1 1 1 0 1 7 3 7 1 8 1 1 0 0 6 1\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_59912]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     11\u001b[0m         train_generator,\n\u001b[1;32m     12\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mEpochs_number,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Train the model without data balancing and specified callbacks\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEpochs_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msave_model_after_each_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_reduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_tensorboard_callback_accuracy\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load the test dataset from the directory using the specified configurations\u001b[39;00m\n\u001b[1;32m     31\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mstr\u001b[39m(test_directory),\n\u001b[1;32m     33\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Automatically infer labels from directory names\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/0d/6qdp6tjj2fj9mgn8w3n53sq40000gn/T/ipykernel_50689/1450207562.py\", line 21, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 359, in _compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 327, in compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 611, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 652, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/loss.py\", line 60, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 1870, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py\", line 1559, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py\", line 671, in sparse_categorical_crossentropy\n\nReceived a label value of 8 which is outside the valid range of [0, 1).  Label values: 6 0 6 1 1 8 5 4 6 1 1 0 5 4 0 8 1 1 1 0 1 7 3 7 1 8 1 1 0 0 6 1\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_59912]"
     ]
    }
   ],
   "source": [
    "# List of metrics to monitor during training\n",
    "metrics = ['val_accuracy', 'accuracy', 'loss', 'val_loss']\n",
    "\n",
    "# Creating an instance of the CustomTensorBoardCallback_Accuracy class\n",
    "custom_tensorboard_callback_accuracy = CustomTensorBoardCallback_Accuracy(metrics)\n",
    "\n",
    "# Train the model with or without data balancing\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    # Train the model with data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights_dict,  # Apply class weights during training\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "else:\n",
    "    # Train the model without data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "\n",
    "# Load the test dataset from the directory using the specified configurations\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(test_directory),\n",
    "    labels='inferred',  # Automatically infer labels from directory names\n",
    "    shuffle=False,  # Keep the test dataset order fixed\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset using the trained model\n",
    "rounded_predictions2 = []\n",
    "file_paths = []\n",
    "\n",
    "for images, labels in test_dataset:  # Use images and labels in batches\n",
    "    predictions = model.predict(images)  # Get predictions for each batch of images\n",
    "    rounded_predictions = np.argmax(predictions, axis=1)  # Get the class index with the highest probability\n",
    "    rounded_predictions2.append(rounded_predictions)\n",
    "    file_paths.extend(test_dataset.file_paths)  # Keep track of the file paths\n",
    "\n",
    "rounded_predictions2 = np.concatenate(rounded_predictions2)  # Combine predictions into a single array\n",
    "\n",
    "# Map predicted class indices to their corresponding class names\n",
    "class_nameslist = [classes[i] for i in rounded_predictions2]\n",
    "\n",
    "# Get file paths and true class labels\n",
    "file_pathss = pd.DataFrame(file_paths, columns=['Path'])  # Create a DataFrame for file paths\n",
    "true_labels = [path.split(os.sep)[-2] for path in file_paths]  # Extract the true class labels from file paths\n",
    "True_class_list = pd.DataFrame(true_labels, columns=['True Class'])  # Create a DataFrame for true class labels\n",
    "\n",
    "# Combine predictions and true labels into a DataFrame\n",
    "classification_result = pd.concat([file_pathss, pd.DataFrame(class_nameslist, columns=['Predicted Class']), True_class_list], axis=1)\n",
    "\n",
    "# Identify misclassified samples\n",
    "misclassified_result = classification_result[classification_result['Predicted Class'] != classification_result['True Class']]\n",
    "\n",
    "# Save misclassified samples to an Excel file\n",
    "filename = f\"{Save_model_at}/{Model_name}_Split_Part_{selected_split_part}_from_{num_split_parts}_TestData.xlsx\"\n",
    "writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "misclassified_result.to_excel(writer, sheet_name='Misclassified_Test_Data', index=False)\n",
    "writer.save()\n",
    "\n",
    "print(f\"Misclassified samples saved to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "2024-11-08 11:45:29.886234: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at sparse_xent_op.cc:103 : INVALID_ARGUMENT: Received a label value of 8 which is outside the valid range of [0, 1).  Label values: 8 1 1 0 0 0 1 0 8 3 1 8 7 0 1 8 1 1 1 3 1 1 7 2 6 8 6 2 6 7 1 7\n",
      "2024-11-08 11:45:29.886275: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Received a label value of 8 which is outside the valid range of [0, 1).  Label values: 8 1 1 0 0 0 1 0 8 3 1 8 7 0 1 8 1 1 1 3 1 1 7 2 6 8 6 2 6 7 1 7\n",
      "\t [[{{function_node __inference_one_step_on_data_58397}}{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/0d/6qdp6tjj2fj9mgn8w3n53sq40000gn/T/ipykernel_50689/1450207562.py\", line 21, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 359, in _compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 327, in compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 611, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 652, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/loss.py\", line 60, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 1870, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py\", line 1559, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py\", line 671, in sparse_categorical_crossentropy\n\nReceived a label value of 8 which is outside the valid range of [0, 1).  Label values: 8 1 1 0 0 0 1 0 8 3 1 8 7 0 1 8 1 1 1 3 1 1 7 2 6 8 6 2 6 7 1 7\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_59912]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     11\u001b[0m         train_generator,\n\u001b[1;32m     12\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mEpochs_number,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Train the model without data balancing and specified callbacks\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEpochs_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msave_model_after_each_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_reduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_tensorboard_callback_accuracy\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load the test dataset from the directory using the specified configurations\u001b[39;00m\n\u001b[1;32m     31\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mstr\u001b[39m(test_directory),\n\u001b[1;32m     33\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Automatically infer labels from directory names\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits defined at (most recent call last):\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py\", line 87, in _run_code\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n\n  File \"/var/folders/0d/6qdp6tjj2fj9mgn8w3n53sq40000gn/T/ipykernel_50689/1450207562.py\", line 21, in <module>\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 359, in _compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/trainer.py\", line 327, in compute_loss\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 611, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/trainers/compile_utils.py\", line 652, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/loss.py\", line 60, in __call__\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/losses/losses.py\", line 1870, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/ops/nn.py\", line 1559, in sparse_categorical_crossentropy\n\n  File \"/Users/vighneshms/Library/Python/3.9/lib/python/site-packages/keras/src/backend/tensorflow/nn.py\", line 671, in sparse_categorical_crossentropy\n\nReceived a label value of 8 which is outside the valid range of [0, 1).  Label values: 8 1 1 0 0 0 1 0 8 3 1 8 7 0 1 8 1 1 1 3 1 1 7 2 6 8 6 2 6 7 1 7\n\t [[{{node compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_one_step_on_iterator_59912]"
     ]
    }
   ],
   "source": [
    "# List of metrics to monitor during training\n",
    "metrics = ['val_accuracy', 'accuracy', 'loss', 'val_loss']\n",
    "\n",
    "# Creating an instance of the CustomTensorBoardCallback_Accuracy class\n",
    "custom_tensorboard_callback_accuracy = CustomTensorBoardCallback_Accuracy(metrics)\n",
    "\n",
    "# Train the model with or without data balancing\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    # Train the model with data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights_dict,  # Apply class weights during training\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "else:\n",
    "    # Train the model without data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "\n",
    "# Load the test dataset from the directory using the specified configurations\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(test_directory),\n",
    "    labels='inferred',  # Automatically infer labels from directory names\n",
    "    shuffle=False,  # Keep the test dataset order fixed\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset using the trained model\n",
    "rounded_predictions2 = []\n",
    "file_paths = []\n",
    "\n",
    "for images, labels in test_dataset:  # Use images and labels in batches\n",
    "    predictions = model.predict(images)  # Get predictions for each batch of images\n",
    "    rounded_predictions = np.argmax(predictions, axis=1)  # Get the class index with the highest probability\n",
    "    rounded_predictions2.append(rounded_predictions)\n",
    "    file_paths.extend(test_dataset.file_paths)  # Keep track of the file paths\n",
    "\n",
    "rounded_predictions2 = np.concatenate(rounded_predictions2)  # Combine predictions into a single array\n",
    "\n",
    "# Map predicted class indices to their corresponding class names\n",
    "class_nameslist = [classes[i] for i in rounded_predictions2]\n",
    "\n",
    "# Get file paths and true class labels\n",
    "file_pathss = pd.DataFrame(file_paths, columns=['Path'])  # Create a DataFrame for file paths\n",
    "true_labels = [path.split(os.sep)[-2] for path in file_paths]  # Extract the true class labels from file paths\n",
    "True_class_list = pd.DataFrame(true_labels, columns=['True Class'])  # Create a DataFrame for true class labels\n",
    "\n",
    "# Combine predictions and true labels into a DataFrame\n",
    "classification_result = pd.concat([file_pathss, pd.DataFrame(class_nameslist, columns=['Predicted Class']), True_class_list], axis=1)\n",
    "\n",
    "# Identify misclassified samples\n",
    "misclassified_result = classification_result[classification_result['Predicted Class'] != classification_result['True Class']]\n",
    "\n",
    "# Save misclassified samples to an Excel file\n",
    "filename = f\"{Save_model_at}/{Model_name}_Split_Part_{selected_split_part}_from_{num_split_parts}_TestData.xlsx\"\n",
    "writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "misclassified_result.to_excel(writer, sheet_name='Misclassified_Test_Data', index=False)\n",
    "writer.save()\n",
    "\n",
    "print(f\"Misclassified samples saved to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Adam' object has no attribute 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 21\u001b[0m\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     11\u001b[0m         train_generator,\n\u001b[1;32m     12\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mEpochs_number,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39m[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Train the model without data balancing and specified callbacks\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEpochs_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msave_model_after_each_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_reduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_tensorboard_callback_accuracy\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load the test dataset from the directory using the specified configurations\u001b[39;00m\n\u001b[1;32m     31\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage_dataset_from_directory(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mstr\u001b[39m(test_directory) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m     37\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[69], line 17\u001b[0m, in \u001b[0;36mCustomTensorBoardCallback_Accuracy.on_epoch_begin\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_epoch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, epoch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Adding learning rates at the beginning of each epoch\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rates\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Adam' object has no attribute 'lr'"
     ]
    }
   ],
   "source": [
    "# List of metrics to monitor during training\n",
    "metrics = ['val_accuracy', 'accuracy', 'loss', 'val_loss']\n",
    "\n",
    "# Creating an instance of the CustomTensorBoardCallback_Accuracy class\n",
    "custom_tensorboard_callback_accuracy = CustomTensorBoardCallback_Accuracy(metrics)\n",
    "\n",
    "# Train the model with or without data balancing\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    # Train the model with data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights_dict,  # Apply class weights during training\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "else:\n",
    "    # Train the model without data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "\n",
    "# Load the test dataset from the directory using the specified configurations\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    str(test_directory) + \"/\",\n",
    "    labels=None,\n",
    "    shuffle=False,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset using the trained model\n",
    "rounded_predictions2 = []\n",
    "for image in test_dataset:\n",
    "    predictions = model.predict(image)  # Get predictions for each image in the test dataset\n",
    "    rounded_predictions = np.argmax(predictions, axis=1)  # Get the class index with the highest probability\n",
    "    rounded_predictions2.append(rounded_predictions)  # Append the predictions to a list\n",
    "rounded_predictions2 = np.concatenate(rounded_predictions2)  # Concatenate all predictions into a single array\n",
    "\n",
    "# Map predicted class indices to their corresponding class names\n",
    "class_nameslist = []\n",
    "for i in rounded_predictions2:\n",
    "    class_nameslist.append(classes[i])  # Map the predicted indices to their respective class names\n",
    "\n",
    "# Get file paths and true class labels for the test dataset\n",
    "XX = test_dataset.file_paths\n",
    "file_pathss = pd.DataFrame(XX, columns=['Path'])  # Create a DataFrame for file paths\n",
    "unique_word_list = []\n",
    "for path in XX:\n",
    "    path = path.replace(\"/\", \"\\\\\")\n",
    "    unique_word = path.split(\"\\\\\")[-2]  # Extract the true class label from the file path\n",
    "    unique_word_list.append(unique_word)  # Append true class labels to a list\n",
    "True_class_lsit = pd.DataFrame(unique_word_list, columns=['True Class'])  # Create a DataFrame for true class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for predicted and true class labels along with file paths\n",
    "class_nameslist = pd.DataFrame(class_nameslist, columns=['Predicted Class'])\n",
    "Classification_result = pd.concat([file_pathss, class_nameslist, True_class_lsit], axis=1)\n",
    "\n",
    "# Identify misclassified samples by comparing predicted and true class labels\n",
    "misclassified_result = Classification_result[\n",
    "    Classification_result['Predicted Class'] != Classification_result['True Class']]\n",
    "\n",
    "# Save misclassified samples to an Excel file\n",
    "filename = (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(\n",
    "    num_split_parts) + \"_TestData.xlsx\")\n",
    "writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "misclassified_result.to_excel(writer, sheet_name='Misclassified_Test_Data', index=False)  # Write misclassified data to Excel\n",
    "writer.save()  # Save the Excel file\n",
    "\n",
    "files.download(filename)  # Download the Excel file\n",
    "print(\"************************ Test Dataset classification is completed and saved.\")  # Print completion message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Classification/Decompressed_dataset/Elsafty_RBCs_for_Segmentation_and_Detection_Slide_1 (1)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m class_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# List all files in the class folder and shuffle their order\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m allfiles \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Shuffle images before splitting\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(Shuffle_Before_Split)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Classification/Decompressed_dataset/Elsafty_RBCs_for_Segmentation_and_Detection_Slide_1 (1)'"
     ]
    }
   ],
   "source": [
    "# Organize images into training, validation, and test directories.\n",
    "# Iterate through each class and distribute images based on the specified number of split parts.\n",
    "### Please note that on selecting the option \"Shuffle_Before_Split\", the total images of each class in its folder will be randomly shuffled.\n",
    "### Subsequently, these shuffled images will be divided into a number of fully separated parts.\n",
    "### One-part will be allocated to testing, the second to validation, and the remaining part(s) to training.\n",
    "### Therefore, no cell will be present in more than one-part, and each part will contain images from each slide/patient.\n",
    "### Moreover, the shuffling will be performed with a fixed seed \"order\" to ensure consistency when choosing different parts for testing and validation.\n",
    "### Thus, the code will split the dataset with no data mix-up or allocating images from certain slides/patients to be specific for certain subset.\n",
    "### This method is useful for exploring data consistency but is not ideal for generalizing performance.\n",
    "### Conversely, without shuffling, the splitting will result in better performance generalization because validation and testing will be conducted on different cases.\n",
    "for cls in classes:\n",
    "    # Create directories for each class within 'training_directory', 'val_directory', and 'test_directory'\n",
    "    os.makedirs(os.path.join(training_directory + \"/\", cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(val_directory + \"/\", cls), exist_ok=True)\n",
    "    os.makedirs(os.path.join(test_directory + \"/\", cls), exist_ok=True)\n",
    "\n",
    "    # Define the folder containing images for the current class\n",
    "    class_folder = os.path.join(root, cls)\n",
    "\n",
    "    # List all files in the class folder and shuffle their order\n",
    "    allfiles = os.listdir(class_folder)\n",
    "\n",
    "    # Shuffle images before splitting\n",
    "    if str(Shuffle_Before_Split).lower() != \"no\":\n",
    "        random.seed(1234)\n",
    "        random.shuffle(allfiles)\n",
    "\n",
    "    # Divide images into parts\n",
    "    part_length = len(allfiles) // num_split_parts\n",
    "    remainder = len(allfiles) % num_split_parts\n",
    "    start_index = 0\n",
    "    result = []\n",
    "\n",
    "    # Create split parts with approximately equal sizes\n",
    "    for _ in range(num_split_parts):\n",
    "        end_index = start_index + part_length\n",
    "        if remainder > 0:\n",
    "            end_index += 1\n",
    "            remainder -= 1\n",
    "        result.append(allfiles[start_index:end_index])\n",
    "        start_index = end_index\n",
    "\n",
    "    # Determine test and validation indices based on selected part\n",
    "    test_inx = selected_split_part - 1\n",
    "    val_inx = selected_split_part\n",
    "    if val_inx >= num_split_parts:\n",
    "        val_inx = 0\n",
    "    test_files = result[test_inx]\n",
    "    val_files = result[val_inx]\n",
    "\n",
    "    # Separate files for training\n",
    "    new_lst = [x for x in allfiles if x not in test_files]\n",
    "    train_files = [x for x in new_lst if x not in val_files]\n",
    "\n",
    "    # Copy images to respective directories for training, validation, and test\n",
    "    for file in test_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(test_directory + \"/\", cls, file))\n",
    "    for file in val_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(val_directory + \"/\", cls, file))\n",
    "    for file in train_files:\n",
    "        shutil.copy(os.path.join(class_folder, file), os.path.join(training_directory + \"/\", cls, file))\n",
    "    print(\"bring class\", cls)  # Print the class name to indicate progress\n",
    "\n",
    "# Function to count the number of files in a directory\n",
    "def count_files(folder_path):\n",
    "    count = 0\n",
    "    for root, dirs, allfiles in os.walk(folder_path):\n",
    "        count += len(allfiles)\n",
    "    return count\n",
    "\n",
    "# Calculate and print the number of files in the training directory\n",
    "folder_path = training_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Training allfiles =\", file_count)\n",
    "\n",
    "# Calculate and print the number of files in the validation directory\n",
    "folder_path = val_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Validation allfiles =\", file_count)\n",
    "\n",
    "# Calculate and print the number of files in the test directory\n",
    "folder_path = test_directory\n",
    "file_count = count_files(folder_path)\n",
    "print(\"Test allfiles =\", file_count)\n",
    "\n",
    "# Display a separator for clarity\n",
    "print(\"#######################################################################\")\n",
    "\n",
    "# Define the size of the images\n",
    "image_size = (image_height, image_width)\n",
    "# Seed for data reproducibility\n",
    "seed = 123\n",
    "\n",
    "# Define data augmentation for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=360,  # Rotate images up to 360 degrees\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    vertical_flip=True  # Flip images vertically\n",
    ")\n",
    "\n",
    "# Data generators for the validation, test, and full data sets are initialized without augmentation\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "fullData_datagen = ImageDataGenerator()\n",
    "\n",
    "# Generating data batches from the directory for training, validation, and test sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_directory,  # Directory containing training images\n",
    "    target_size=image_size,  # Resize images to match 'image_size'\n",
    "    batch_size=batch_size,  # Number of samples per gradient update\n",
    "    class_mode='sparse',  # Mode for class labels (integer)\n",
    "    seed=seed  # Seed for shuffling\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_directory,  # Directory containing validation images\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_directory,  # Directory containing test images\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='sparse',\n",
    "    shuffle=False  # Do not shuffle the test data during inference\n",
    ")\n",
    "\n",
    "# Use EfficientNetB0 as the base model for transfer learning\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    weights='imagenet',  # Initialize with pre-trained ImageNet weights\n",
    "    include_top=False,  # Exclude the fully connected layers from the top\n",
    "    input_shape=(image_height, image_width, 3)  # Define input image dimensions and channels\n",
    ")\n",
    "\n",
    "# Freeze the pre-trained layers to prevent updating during training\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True  # Setting the layers to trainable\n",
    "\n",
    "# Add a custom classification head on top of the pre-trained model\n",
    "preprocess_input = tf.keras.applications.efficientnet.preprocess_input  # Preprocessing input function for EfficientNet\n",
    "inputs = tf.keras.Input(shape=(image_height, image_width, 3))  # Define input shape\n",
    "x = preprocess_input(inputs)  # Preprocess the input\n",
    "x = base_model(x, training=False)  # Utilize the base model without training it\n",
    "x = layers.GlobalAveragePooling2D()(x)  # Global average pooling layer\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  # Dropout layer with dropout rate of 0.2\n",
    "outputs = tf.keras.layers.Dense(num_classes)(x)  # Dense layer for classification\n",
    "model = tf.keras.Model(inputs, outputs)  # Create the final model\n",
    "\n",
    "# Display the model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Compile the model with specified optimizer, loss, and metrics\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_LearningRate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[\n",
    "        tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # Sparse categorical cross-entropy loss\n",
    "    ],\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')  # Sparse categorical accuracy metric\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save the model after each epoch if it performs better\n",
    "save_model_after_each_epoch = tf.keras.callbacks.ModelCheckpoint(\n",
    "    (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(num_split_parts) + '.h5'),\n",
    "    verbose=1,\n",
    "    save_best_only=True  # Save only the best model\n",
    ")\n",
    "\n",
    "# Define the learning rate reduction callback based on validation loss\n",
    "lr_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=Patience,  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1,\n",
    "    min_delta=min_delta  # Minimum change in the monitored quantity to qualify as an improvement\n",
    ")\n",
    "\n",
    "# Compute class weights based on the training dataset distribution\n",
    "class_counts = np.bincount(train_generator.classes)  # Count occurrences of each class in the training set\n",
    "total_samples = sum(class_counts)  # Calculate the total number of samples\n",
    "class_weights = [total_samples / (len(classes) * count) for count in class_counts]  # Compute class weights\n",
    "class_weights_dict = {class_label: weight for class_label, weight in enumerate(class_weights)}  # Create a dictionary of class weights\n",
    "\n",
    "# Check if data balancing is required and print class counts and weights if 'yes'\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    print(class_counts)\n",
    "    print(class_weights)\n",
    "\n",
    "# Train the model using the configured generators and settings\n",
    "steps_per_epoch = train_generator.samples // train_generator.batch_size  # Calculate steps per epoch\n",
    "validation_steps = validation_generator.samples // validation_generator.batch_size  # Calculate validation steps\n",
    "\n",
    "# Define TensorBoard\n",
    "class CustomTensorBoardCallback_Accuracy(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        super(CustomTensorBoardCallback_Accuracy, self).__init__()\n",
    "        # Initializing the callback with a list of metrics to monitor\n",
    "        self.metrics = metrics\n",
    "        # Creating empty lists to store metric values, epochs, and learning rates\n",
    "        self.metric_values = {metric: [] for metric in metrics}\n",
    "        self.epochs = []\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        pass  # No action needed when the training starts\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Adding learning rates at the beginning of each epoch\n",
    "        self.learning_rates.append(self.model.optimizer.lr.numpy())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.epochs.append(epoch + 1)  # Incrementing the epoch number by 1\n",
    "        for metric in self.metrics:\n",
    "            # Collecting metric values at the end of each epoch\n",
    "            self.metric_values[metric].append(logs.get(metric))\n",
    "\n",
    "        # Plotting metrics and learning rates after each epoch\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))  # Creating a plot figure\n",
    "        lines = []\n",
    "        colors = ['blue', 'orange', 'green', 'red']  # Colors for different metrics\n",
    "\n",
    "        # Plotting each metric curve\n",
    "        for i, metric in enumerate(self.metrics):\n",
    "            line, = ax1.plot(self.epochs, self.metric_values[metric], '-', color=colors[i], label=metric)\n",
    "            lines.append(line)\n",
    "\n",
    "        # Adding a line for the learning rate\n",
    "        lr_line, = ax1.plot(self.epochs, self.learning_rates, '--', label='Learning Rate')\n",
    "\n",
    "        # Setting up plot labels, title, ticks, and legend\n",
    "        ax1.set_ylabel('Value')\n",
    "        ax1.set_title('Epoch')\n",
    "        ax1.set_xticks(np.arange(min(self.epochs), max(self.epochs) + 1, 1))\n",
    "        ax1.set_ylim(0, 1.0)\n",
    "        ax1.set_yticks(np.arange(0, 1.1, 0.05))\n",
    "        ax1.legend(lines + [lr_line], self.metrics + ['Learning Rate'])\n",
    "        ax1.grid(True)\n",
    "        ax1.tick_params(axis='x', top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "\n",
    "        # Adding a secondary y-axis for better readability\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylim(0, 1.0)\n",
    "        ax2.set_yticks(np.arange(0, 1.1, 0.05))\n",
    "        ax2.yaxis.tick_right()\n",
    "        ax2.yaxis.set_label_position(\"right\")\n",
    "        ax2.set_ylabel('Value')\n",
    "\n",
    "        # Adding text labels for curves with an offset\n",
    "        offset = 0.02\n",
    "        for i in range(len(self.epochs)):\n",
    "            x = self.epochs[i]\n",
    "            y_lr = self.learning_rates[i]\n",
    "            y_val_loss = self.metric_values['val_loss'][i]\n",
    "            y_val_acc = self.metric_values['val_accuracy'][i]\n",
    "            lr_label = f'{y_lr:.1e}'\n",
    "            lr_label = (lr_label.replace(\"e-0\", \"e-\").replace(\"0e\", \"e\"))[2:]\n",
    "            val_loss_label = str(int(y_val_loss * 100))\n",
    "            val_acc_label = str(int(y_val_acc * 100))\n",
    "            ax1.text(x, y_lr - (offset * 4), lr_label, ha='center', va='bottom', color=lr_line.get_color())\n",
    "            ax1.text(x, y_val_loss - (offset * 3), val_loss_label, ha='center', va='bottom', color=colors[3])\n",
    "            ax1.text(x, y_val_acc + offset, val_acc_label, ha='center', va='bottom', color=colors[0])\n",
    "        plt.show()\n",
    "\n",
    "# List of metrics to monitor during training\n",
    "metrics = ['val_accuracy', 'accuracy', 'loss', 'val_loss']\n",
    "\n",
    "# Creating an instance of the CustomTensorBoardCallback_Accuracy class\n",
    "custom_tensorboard_callback_accuracy = CustomTensorBoardCallback_Accuracy(metrics)\n",
    "\n",
    "# Train the model with or without data balancing\n",
    "if str(Give_data_balance).lower() == \"yes\":\n",
    "    # Train the model with data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights_dict,  # Apply class weights during training\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "else:\n",
    "    # Train the model without data balancing and specified callbacks\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=Epochs_number,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=[save_model_after_each_epoch, lr_reduction, custom_tensorboard_callback_accuracy]\n",
    "    )\n",
    "\n",
    "# Load the test dataset from the directory using the specified configurations\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    str(test_directory) + \"/\",\n",
    "    labels=None,\n",
    "    shuffle=False,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset using the trained model\n",
    "rounded_predictions2 = []\n",
    "for image in test_dataset:\n",
    "    predictions = model.predict(image)  # Get predictions for each image in the test dataset\n",
    "    rounded_predictions = np.argmax(predictions, axis=1)  # Get the class index with the highest probability\n",
    "    rounded_predictions2.append(rounded_predictions)  # Append the predictions to a list\n",
    "rounded_predictions2 = np.concatenate(rounded_predictions2)  # Concatenate all predictions into a single array\n",
    "\n",
    "# Map predicted class indices to their corresponding class names\n",
    "class_nameslist = []\n",
    "for i in rounded_predictions2:\n",
    "    class_nameslist.append(classes[i])  # Map the predicted indices to their respective class names\n",
    "\n",
    "# Get file paths and true class labels for the test dataset\n",
    "XX = test_dataset.file_paths\n",
    "file_pathss = pd.DataFrame(XX, columns=['Path'])  # Create a DataFrame for file paths\n",
    "unique_word_list = []\n",
    "for path in XX:\n",
    "    path = path.replace(\"/\", \"\\\\\")\n",
    "    unique_word = path.split(\"\\\\\")[-2]  # Extract the true class label from the file path\n",
    "    unique_word_list.append(unique_word)  # Append true class labels to a list\n",
    "True_class_lsit = pd.DataFrame(unique_word_list, columns=['True Class'])  # Create a DataFrame for true class labels\n",
    "\n",
    "# Create DataFrames for predicted and true class labels along with file paths\n",
    "class_nameslist = pd.DataFrame(class_nameslist, columns=['Predicted Class'])\n",
    "Classification_result = pd.concat([file_pathss, class_nameslist, True_class_lsit], axis=1)\n",
    "\n",
    "# Identify misclassified samples by comparing predicted and true class labels\n",
    "misclassified_result = Classification_result[\n",
    "    Classification_result['Predicted Class'] != Classification_result['True Class']]\n",
    "\n",
    "# Save misclassified samples to an Excel file\n",
    "filename = (str(Save_model_at) + \"/\" + str(Model_name) + \"_Split_Part_\" + str(selected_split_part) + \"_from_\" + str(\n",
    "    num_split_parts) + \"_TestData.xlsx\")\n",
    "writer = pd.ExcelWriter(filename, engine='xlsxwriter')\n",
    "misclassified_result.to_excel(writer, sheet_name='Misclassified_Test_Data', index=False)  # Write misclassified data to Excel\n",
    "writer.save()  # Save the Excel file\n",
    "\n",
    "files.download(filename)  # Download the Excel file\n",
    "print(\"************************ Test Dataset classification is completed and saved.\")  # Print completion message"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
